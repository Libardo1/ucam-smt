/*!
  \page glossary Glossary 

<a name="alignment_lattices"><b>Alignment lattices/FSTs:</b></a> FSTs generated by HiFST that contain derivations (i.e. sequences of rule indices) on the left side or input tape and translation hypotheses (i.e. sequences of words)  on the right side or output tape.

<a name="cell_fsa"><b>Cell FSA:</b></a> An FSA that describes the target search space of a CYK cell.

<a name="cell_fsa_pruning"><b>Cell FSA pruning:</b></a> Pruning strategy used in HiFST during RTN construction. This strategy is in general local. However, under hiero grammars, for a sentence of n words, cell FSA (S,0,n) covers the complete search space and therefore pruning in this particular cell is exact.

<a name="cyk_cell"><b>CYK cell:</b></a> When source sentences are parsed, candidate rules are clustered in cells. In a hiero decoder, these cells have three dimensions: the LHS or head of the rule, the position in the source sentence and the span. We can express this compactly as (LHS,position,span). For instance, (X,3,5) refers to all candidate rules headed by X, covering 5 words in position 3.


<a name="exact_pruning"><b>Exact pruning:</b></a> Pruning that is exact under the model and a particular semiring â€“ this is, there are no search errors and therefore hypotheses falling inside the beam (or an n-best) are guaranteed to exist in the pruned FSA. 

<a name="flower_shaped"><b>Flower-shaped FSA/FST:</b></a> A cyclic FSA/FST with one single start/exit state. All arcs depart from and arrive to this state. 

<a name="fsa"><b>FSA, FST:</b></a> Finite-state automata or transducers, respectively. Check www.openfst.org.

<a name="grammar_patterns"><b>Grammar patterns:</b></a> A pattern is a rule in which we rewrite subsequences of words into a common character. For example, consider the rule  X X1_3_X2_5  10_X1_X2_8. The pattern is X X1_w_X2_w w_X1_X2_w.

<a name="local_pruning"><b>Local pruning:</b></a> Refers to the HiFST pruning strategy during RTN construction. Any of the CYK cell FSAs could be pruned as long as the source word span and the number of states of the FSA are over certain thresholds.

<a name="lexicographic_semiring"><b>Lexicographic semiring:</b></a> This is a short term for a lexicographic semiring over two tropical weights.  See [LINK]Roark's paper[/LINK] for alternative lexicographic semirings. In a nutshell, a lexicographic semiring is equivalent to a tropical semiring over the first weight. The second weight is a tie breaker. Using this semiring allows for efficient language model removal. Assume that we have a grammar score G and a language model score M in a particular arc. The first weight encodes both contributions, and we leave G in the second weight: <G+M,G>. In order to remove language model scores, we only need to copy the second weight into the first weight, this is, <G,G>.

<a name="pda"><b>PDA,PDT:</b></a> push-down automata or transducers, respectively. Check  http://www.openfst.org/twiki/bin/view/FST/FstExtensions.

<a name="reference_lattice"><b>Reference lattice/FSA:</b></a>   FSAs used to guide HiFST decoding. In general, we use translation lattices.

<a name="rtn"><b>RTN:</b></a> Stands for Recursive Transition Network. It is a list of FSAs accepting words and special symbols that act as pointers to other FSAs in this list. Check Replace operation in www.openfst.org.

<a name="tropical_sparse_tuple_semiring"><b>Tropical sparse tuple semiring:</b></a> This is a tropical semiring ( see http://www.openfst.org/twiki/bin/view/FST/FstQuickTour#FstWeights ) in which individual unscaled feature contribution are kept in the arc. <br>
Let us assume a simple example of two arcs with two respective feature weights f11, f12 and f21, f22. Scaling factors are s1, s2. Then,

	 w1=Times(s1 * f11 , s2 *f12) = s1* f11 + s2* f12
	 w2=Times(s1 * f21, s2 *f22) = s1 * f21 + s2 * f22

w1 and w2 are weights under the tropical semiring, this is:

	Times(w1,w2) = w1 + w2
	Plus(w1,w2) = min(w1,w2).

The features are represented internally with tuples (feature_index,feature_weight), hence ideal for sparse feature representation.

*/